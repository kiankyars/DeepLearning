
class BasicTokenizer:
    def __init__(self, text, vocab_size):
        assert vocab_size > 255
        self.vocab_size = vocab_size
    def train(self, text, vocab_size, verbose=False):
    def encode(self, text):
        
    def decode(self, ids):
        pass
    def _merge(self):
        pass